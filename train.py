import torch
import argparse
import os
from unsloth import FastLanguageModel
from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset
from peft import LoraConfig
from parser import KMMLUArgumentManager

# 1. Argparser ì„¤ì • ë° ì¸ì íŒŒì‹±
def parse_args() -> argparse.Namespace:
    parser = KMMLUArgumentManager.get_train_parser()
    args = parser.parse_args()
    return args

# 2. LoRA ì„¤ì • ì •ì˜ í•¨ìˆ˜ (Argparse ì¸ìë¥¼ ì‚¬ìš©)
def create_lora_config(args: argparse.Namespace) -> LoraConfig:
    # ëª¨ë“  Attention ë° FFN ë ˆì´ì–´ì— LoRA ì ìš© (ì„±ëŠ¥ í–¥ìƒ ëª©ì )
    target_modules = [
        "q_proj", "v_proj", "o_proj", "k_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
    
    lora_config = LoraConfig(
        r = args.lora_r,
        lora_alpha = args.lora_alpha,
        target_modules = target_modules,
        lora_dropout = args.lora_dropout,
        bias = "none",
        task_type = "CAUSAL_LM",
    )
    return lora_config

# 3. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ (Unsloth QLoRA ëª¨ë“œ)
def load_unsloth_model(args: argparse.Namespace, lora_config: LoraConfig):
    print(f"ëª¨ë¸ ë¡œë”© ì¤‘: {args.model_id} (QLoRA 4bit)")
    
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = args.model_id,
        max_seq_length = args.max_seq_length,
        dtype = None,           # ìë™ìœ¼ë¡œ ìµœì ì˜ dtype (bf16) ì„ íƒ
        load_in_4bit = True,    # QLoRA (4bit ì–‘ìí™”) ì ìš©
    )
    
    # LoRA Configë¥¼ ëª¨ë¸ì— ì ìš© (PEFT)
    model = FastLanguageModel.get_peft_model(
        model,
        r = lora_config.r,                           # r ê°’ ì „ë‹¬
        target_modules = lora_config.target_modules, # target_modules ì „ë‹¬
        lora_alpha = lora_config.lora_alpha,         # lora_alpha ì „ë‹¬
        lora_dropout = lora_config.lora_dropout,     # lora_dropout ì „ë‹¬
        bias = lora_config.bias,                     # bias ì „ë‹¬
        use_gradient_checkpointing = "unsloth",      # ë©”ëª¨ë¦¬ ì ˆì•½ ê¸°ëŠ¥ í™œì„±í™”
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    return model, tokenizer

# 4. ë°ì´í„°ì…‹ ë¡œë“œ ë° í¬ë§·íŒ… (ì‚¬ìš©ìê°€ êµ¬ì¶•í•œ Instruction Dataì…‹ì— ë§ì¶° ë³€ê²½ í•„ìš”)
def load_and_format_data(tokenizer):
    # ğŸš¨ğŸš¨ ì£¼ì˜: ì´ ë°ì´í„°ì…‹ì€ Alpaca ì˜ˆì‹œì´ë©°, ì‹¤ì œë¡œëŠ” KMMLU ì·¨ì•½ ë¶„ì•¼ ë°ì´í„°ë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤. 
    print("ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘...")
    dataset = load_dataset("nayohan/math-gpt-4o-200k-ko", split="train[:1000]") 

    # Instruction Tuning í¬ë§· í•¨ìˆ˜ (Qwen Instruct í¬ë§·)
    def formatting_prompts_func(examples):
        texts = []
        # kmmlu datasetìš©
        # for instruction, input_text, output_text in zip(examples["instruction"], examples["input"], examples["output"]):
        #     prompt = f"### Instruction:\n{instruction}\n\n"
        #     if input_text:
        #         prompt += f"### Input:\n{input_text}\n\n"
        #     prompt += f"### Response:\n{output_text}"
        #     texts.append(prompt)
        
        # nayohan/math-gpt-4o-200k-ko datasetìš©
        for prompt_text, response_text in zip(examples["prompt"], examples["response"]):
            prompt = f"### Instruction:\n{prompt_text}\n\n"
            prompt += f"### Response:\n{response_text}"
            texts.append(prompt)
        
        return {"text": texts}

    dataset = dataset.map(formatting_prompts_func, batched=True)
    return dataset

# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    args = parse_args() 
    lora_config = create_lora_config(args)
    
    model, tokenizer = load_unsloth_model(args, lora_config)
    train_dataset = load_and_format_data(tokenizer)

    # í•™ìŠµ ì¸ì ì„¤ì •
    training_args = TrainingArguments(
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_acc_steps,
        warmup_steps=50,
        num_train_epochs=args.num_epochs,
        learning_rate=args.learning_rate,
        fp16=not torch.cuda.is_available(),
        bf16=torch.cuda.is_available(), # A100 í™˜ê²½ì—ì„œ bf16 ì‚¬ìš©
        logging_steps=1,
        output_dir=args.output_dir,
        optim="adamw_8bit",
        seed=args.seed,
        save_strategy="epoch",
    )

    # SFT Trainer ì„¤ì • ë° í•™ìŠµ ì‹œì‘
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=train_dataset,
        peft_config=lora_config,
        dataset_text_field="text",
        max_seq_length=args.max_seq_length,
        args=training_args,
    )
    
    print("\n" + "="*60)
    print("SFT í•™ìŠµ ì‹œì‘")
    print("="*60 + "\n")

    trainer.train()

    # LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ë° ì €ì¥
    print("\ní•™ìŠµ ì™„ë£Œ! LoRA ê°€ì¤‘ì¹˜ ë³‘í•© ë° ì €ì¥ ì¤‘...")
    output_path = os.path.join(args.output_dir, "merged_model")
    model.save_pretrained_merged(
        output_path, 
        tokenizer, 
        save_method = "merged_4bit_forced",
    )
    print(f"ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_path}")

if __name__ == "__main__":
    main()